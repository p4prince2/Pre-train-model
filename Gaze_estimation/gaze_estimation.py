# -*- coding: utf-8 -*-
"""Gaze estimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X15cv3EYMOiTpb9PWC87n9gM7e3jI0Zs

# Plan:

- **Face Detection:** Use a pre-trained model (YOLOv5) to detect the driverâ€™s face in the video.

- **Gaze Estimation:** Use a pre-trained model for gaze estimation (such as Gaze360 or Mediapipe).



- **Classification Rule:** Based on gaze direction, predict if the driver is "focused" or "not focused". If the driver is looking straight ahead and their eyes are open, they are considered "focused"; otherwise, they are "not focused".

# Step 1: Install Dependencies
"""

!pip install torch torchvision torchaudio --upgrade
!pip install opencv-python
!pip install yacs matplotlib numpy
!git clone https://github.com/ultralytics/yolov5
!pip install -r yolov5/requirements.txt
!git clone https://github.com/yuval-alaluf/gaze360.git
!pip install mediapipe

"""# Step 2: Import Libraries"""

!pip install --upgrade numpy

!pip install --upgrade torch torchvision torchaudio
!pip install --upgrade opencv-python
  ## resart kernal

import numpy as np
import torch
import cv2

print("NumPy version:", np.__version__)
print("PyTorch version:", torch.__version__)
print("OpenCV version:", cv2.__version__)

import torch
import cv2
import numpy as np
from torchvision import models, transforms
import mediapipe as mp

"""# Step 3: Load YOLOv5 for Face Detection
**We'll use YOLOv5 (pre-trained) to detect the face in the video frames:**
"""

# Load YOLOv5 model (pre-trained)
yolo_model = torch.hub.load('/content/yolov5', 'yolov5s', source='local', force_reload=True)

def detect_driver_face(frame):
    results = yolo_model(frame)
    for *box, conf, cls in results.xyxy[0]:
        if int(cls) == 0:  # class 0 = person
            x1, y1, x2, y2 = map(int, box)
            return frame[y1:y2, x1:x2]
    return None

"""# Step 4: Estimate Gaze Using Mediapipe
**We'll use Mediapipe to estimate the driverâ€™s gaze direction.**
"""

# Initialize Mediapipe for face landmarks detection
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5)

def estimate_gaze_vector(frame):
    # Convert frame to RGB for Mediapipe
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = face_mesh.process(frame_rgb)

    if results.multi_face_landmarks:
        landmarks = results.multi_face_landmarks[0]  # Get landmarks for the first face
        # Extract gaze direction from landmarks (simplified example)
        # In practice, this should compute a vector from eye or face orientation
        return np.array([0.1, -0.1, 0.9])  # Dummy gaze vector for illustration
    return np.array([0.0, 0.0, 1.0])  # Default gaze vector when no face is detected

"""# Step 5: Classify "Focused" vs "Not Focused"
**Using the gaze vector (and optionally eye openness), we'll classify whether the driver is focused or not. A simple rule could be based on whether the gaze direction is aligned with the center of the screen, i.e., if the gaze is roughly in the forward direction and the eyes are open, the driver is considered "focused."**
"""

def classify_focus(gaze_vector, face_img):
    # Gaze vector: [x, y, z] - we can use the z-axis for forwardness
    gaze_forward = gaze_vector[2]  # The forwardness of the gaze direction
    # For simplicity, if the gaze is within a certain range, we say the driver is focused
    if gaze_forward > 0.7:  # Adjust threshold based on your data
        return "Focused"
    else:
        return "Not Focused"

"""# Step 6: Process Video and Detect Focused State
***Now, we process the video and detect whether the driver is focused or not.***

# By self
"""

!wget -O dmd-dataset-gaze-gA-1.tar.gz "https://datasets.vicomtech.org/di21-dmd-dataset-gaze/dmd-dataset-gaze-gA-1.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=Cvu9WzjJuNCiu0Ve%2F20250430%2Feu-vicom-cluster03-ovh%2Fs3%2Faws4_request&X-Amz-Date=20250430T085759Z&X-Amz-Expires=172800&X-Amz-SignedHeaders=host&X-Amz-Signature=92bb6357e96099d130e6e0f963d81e2d2c49244173411ff205861d138dbab219"

"""# By Mentor"""

!curl -L -o DAD.part01.rar "https://webdisk.ads.mwn.de/Handlers/AnonymousDownload.ashx?folder=18e2eac4&path=Datenbanken\\DAD.part01.rar"

!apt-get install unrar

!unrar x DAD.part01.rar

import cv2
import os

# Set the path to the folder containing images
image_folder = '/content/DAD/Tester1/messaging_right/front_IR'
video_name = 'output_video.mp4'

# Get list of image files
images = [img for img in os.listdir(image_folder) if img.endswith((".png", ".jpg", ".jpeg"))]
images.sort()  # Sort by name to ensure correct order

# Read the first image to get frame dimensions
frame = cv2.imread(os.path.join(image_folder, images[0]))
height, width, layers = frame.shape

# Define video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec
video = cv2.VideoWriter(video_name, fourcc, 30, (width, height))  # 30 is FPS

# Write each image to the video
for image in images:
    img_path = os.path.join(image_folder, image)
    frame = cv2.imread(img_path)
    video.write(frame)

# Release everything
video.release()
print(f"Video saved as {video_name}")

video_path = "/content/output_video.mp4"  # Upload your video to Colab
cap = cv2.VideoCapture(video_path)
from google.colab.patches import cv2_imshow
import time

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Detect face
    face = detect_driver_face(frame)
    if face is not None:
        # Extract features from ResNet and estimate gaze
        gaze_vector = estimate_gaze_vector(frame)

        # Classify if the driver is focused
        focus_status = classify_focus(gaze_vector, face)
        print(focus_status)  # "Focused" or "Not Focused"

        # Optionally, draw the gaze direction on the frame
        cv2.putText(frame, focus_status, (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)

    # Display the frame with focus status
    cv2_imshow(frame)

    # Wait for 30 ms between frames (simulates 30 FPS)
    time.sleep(0.03)

cap.release()

"""# Key Points:

- Pre-trained YOLOv5 is used for face detection.


- Mediapipe estimates the gaze direction, and based on that, we classify if the driver is focused.

#Classification Rule:

- Based on the gaze vector, the driver is classified as "focused" if their gaze is mostly forward.

# Next Steps:

- Gaze Estimation: You can improve gaze estimation accuracy by using specialized models like Gaze360 instead of Mediapipe.

- Fine-tuning the Thresholds: The threshold used for the focus classification (gaze_forward > 0.7) may need fine-tuning based on your data.
"""

